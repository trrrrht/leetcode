**Problem:**
You are given an integer array `cost` where `cost[i]` is the cost of `ith` step on a staircase. Once you pay the cost, you can either climb one or two steps.

You can either start from the step with index `0`, or the step with index `1`.

Return *the minimum cost to reach the top of the floor*.

 

**Example 1:**

```
Input: cost = [10,15,20]
Output: 15
Explanation: Cheapest is: start on cost[1], pay that cost, and go to the top.
```

**Example 2:**

```
Input: cost = [1,100,1,1,1,100,1,1,100,1]
Output: 6
Explanation: Cheapest is: start on cost[0], and only step on 1s, skipping cost[3].
```

 

**Constraints:**

- `2 <= cost.length <= 1000`
- `0 <= cost[i] <= 999`
**My Solution:**
```
int minCostClimbingStairs(vector<int>& cost) {
        vector<int> costs = {cost[0], cost[1]};
        for (int i = 2; i < cost.size(); i++) {
            costs[i % 2] = min(costs[0] + cost[i], costs[1] + cost[i]);
        }
        return min(costs[0], costs[1]);
    }
```
Correctness:

For each stair, we should consider the minimum steps to step on it(consider the two stairs before it).

Complexity:

Time: O(n)
Space: O(1)